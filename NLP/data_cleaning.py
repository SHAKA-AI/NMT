# -*- coding: utf-8 -*-
"""data_cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SMBZ7YpOIQeeUevtzLz-WOCG3k4R9qUJ
"""

import numpy as np
import pandas as pd
import xml.etree.ElementTree as et
from collections import defaultdict
def parse_XML(target_language, df_cols): 
    """Parse the input XML file and store the result in a pandas DataFrame 
    with the given columns. The first element of df_cols is supposed to be 
    the identifier variable, which is an attribute of each node element in 
    the XML data; other features will be parsed from the text content of 
    each sub-element. """
    
    xtree = et.parse(target_language)
    xroot = xtree.getroot()
    #out_df = pd.DataFrame(columns = df_cols)
    
    data=[]
    for child in xroot.getchildren():
        #if len(data)>100:
            #break;
        if child.getchildren():
            for child1 in child.getchildren():
                 data.append((child1.attrib.get('id'),child1.text))
        else:
            data.append((child.attrib.get('id'), child.text))

            out_df = pd.DataFrame(data)
            out_df.columns = ['id', 'text']
    return out_df

# load the data
source_language_df = parse_XML("raw/source_language.xml", ['id', 'text'])
target_language_df = parse_XML("raw/target_language.xml", ['id', 'text'])
print(source_language_df.head())

"""
INPUT: 
    source(pandas.DataFrame) : source language
    target(pandas.DataFrame) : target language
    match_by(string): columns id to match
    
OUTPUT: 
    pandas.DataFrame containing 'source_id', 'target_id', 'source_text', 'target_text' 
"""
def matched_data(source, target, match_by) : 
    source_ids = source[match_by]
    target_ids = target[match_by]
    
    matched_data = []
        
    for i in range(len(source_ids)) : 
        for j in range(len(target_ids)) : 
            #print(i,j)            
            if source_ids[i] == target_ids[j] : 
                matched_data.append([source_ids[i],target_ids[j], source.text[i], target.text[j]])
    
    columns = ['source_id', 'target_id', 'source_text', 'target_text']
    
    return pd.DataFrame(data=matched_data, columns=columns)

match_by = 'id'
df = matched_data(source_language_df,target_language_df,match_by)
df = pd.DataFrame(df)
print(df.head())

#spliting the data into train,test and validation 
train, valid, test = np.split(df.sample(frac=1), [int(.8*len(df)), int(.9*len(df))])
print('The training dataset has',len(train),'sentences')
print('The test dataset has',len(test),'sentences')
print('The validation dataset has',len(valid),'sentences')
print(valid.head())

train.source_language = train.drop(["target_id", "target_text"],axis=1)
train.target_language = train.drop(["source_id", "source_text"],axis=1)
test.source_language = test.drop(["target_id", "target_text"],axis=1)
test.target_language = test.drop(["source_id", "source_text"],axis=1)
valid.source_language = valid.drop(["target_id", "target_text"],axis=1)
valid.target_language = valid.drop(["source_id", "source_text"],axis=1)

print(train.source_language.head())
print(train.target_language.head())

train.sw = train.source_language.source_text
train.en = train.target_language.target_text
test.sw = test.source_language.source_text
test.en = test.target_language.target_text
valid.sw = valid.source_language.source_text
valid.en = valid.target_language.target_text

train.sw.to_csv("train.sw", sep = '\n', index = False)
train.en.to_csv("train.en", sep = '\n', index = False)
test.sw.to_csv("test.sw", sep = '\n', index = False)
test.en.to_csv("test.en", sep = '\n', index = False)
valid.sw.to_csv("valid.sw", sep = '\n', index = False)
valid.en.to_csv("valid.en", sep = '\n', index = False)

